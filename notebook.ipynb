{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import display\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix, accuracy_score, classification_report\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Set up and explore data\n",
    "* load data set\n",
    "* explore and understand dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "outputs": [
    {
     "data": {
      "text/plain": "   age  sex  cp  trtbps  chol  fbs  restecg  thalachh  exng  oldpeak  slp  \\\n0   63    1   3     145   233    1        0       150     0      2.3    0   \n1   37    1   2     130   250    0        1       187     0      3.5    0   \n2   41    0   1     130   204    0        0       172     0      1.4    2   \n3   56    1   1     120   236    0        1       178     0      0.8    2   \n4   57    0   0     120   354    0        1       163     1      0.6    2   \n\n   caa  thall  output  \n0    0      1       1  \n1    0      2       1  \n2    0      2       1  \n3    0      2       1  \n4    0      2       1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>sex</th>\n      <th>cp</th>\n      <th>trtbps</th>\n      <th>chol</th>\n      <th>fbs</th>\n      <th>restecg</th>\n      <th>thalachh</th>\n      <th>exng</th>\n      <th>oldpeak</th>\n      <th>slp</th>\n      <th>caa</th>\n      <th>thall</th>\n      <th>output</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>63</td>\n      <td>1</td>\n      <td>3</td>\n      <td>145</td>\n      <td>233</td>\n      <td>1</td>\n      <td>0</td>\n      <td>150</td>\n      <td>0</td>\n      <td>2.3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>37</td>\n      <td>1</td>\n      <td>2</td>\n      <td>130</td>\n      <td>250</td>\n      <td>0</td>\n      <td>1</td>\n      <td>187</td>\n      <td>0</td>\n      <td>3.5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>41</td>\n      <td>0</td>\n      <td>1</td>\n      <td>130</td>\n      <td>204</td>\n      <td>0</td>\n      <td>0</td>\n      <td>172</td>\n      <td>0</td>\n      <td>1.4</td>\n      <td>2</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>56</td>\n      <td>1</td>\n      <td>1</td>\n      <td>120</td>\n      <td>236</td>\n      <td>0</td>\n      <td>1</td>\n      <td>178</td>\n      <td>0</td>\n      <td>0.8</td>\n      <td>2</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>57</td>\n      <td>0</td>\n      <td>0</td>\n      <td>120</td>\n      <td>354</td>\n      <td>0</td>\n      <td>1</td>\n      <td>163</td>\n      <td>1</td>\n      <td>0.6</td>\n      <td>2</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heart = pd.read_csv('data/heart.csv')\n",
    "\n",
    "heart.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "outputs": [
    {
     "data": {
      "text/plain": "              age         sex          cp      trtbps        chol         fbs  \\\ncount  303.000000  303.000000  303.000000  303.000000  303.000000  303.000000   \nmean    54.366337    0.683168    0.966997  131.623762  246.264026    0.148515   \nstd      9.082101    0.466011    1.032052   17.538143   51.830751    0.356198   \nmin     29.000000    0.000000    0.000000   94.000000  126.000000    0.000000   \n25%     47.500000    0.000000    0.000000  120.000000  211.000000    0.000000   \n50%     55.000000    1.000000    1.000000  130.000000  240.000000    0.000000   \n75%     61.000000    1.000000    2.000000  140.000000  274.500000    0.000000   \nmax     77.000000    1.000000    3.000000  200.000000  564.000000    1.000000   \n\n          restecg    thalachh        exng     oldpeak         slp         caa  \\\ncount  303.000000  303.000000  303.000000  303.000000  303.000000  303.000000   \nmean     0.528053  149.646865    0.326733    1.039604    1.399340    0.729373   \nstd      0.525860   22.905161    0.469794    1.161075    0.616226    1.022606   \nmin      0.000000   71.000000    0.000000    0.000000    0.000000    0.000000   \n25%      0.000000  133.500000    0.000000    0.000000    1.000000    0.000000   \n50%      1.000000  153.000000    0.000000    0.800000    1.000000    0.000000   \n75%      1.000000  166.000000    1.000000    1.600000    2.000000    1.000000   \nmax      2.000000  202.000000    1.000000    6.200000    2.000000    4.000000   \n\n            thall      output  \ncount  303.000000  303.000000  \nmean     2.313531    0.544554  \nstd      0.612277    0.498835  \nmin      0.000000    0.000000  \n25%      2.000000    0.000000  \n50%      2.000000    1.000000  \n75%      3.000000    1.000000  \nmax      3.000000    1.000000  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>sex</th>\n      <th>cp</th>\n      <th>trtbps</th>\n      <th>chol</th>\n      <th>fbs</th>\n      <th>restecg</th>\n      <th>thalachh</th>\n      <th>exng</th>\n      <th>oldpeak</th>\n      <th>slp</th>\n      <th>caa</th>\n      <th>thall</th>\n      <th>output</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>303.000000</td>\n      <td>303.000000</td>\n      <td>303.000000</td>\n      <td>303.000000</td>\n      <td>303.000000</td>\n      <td>303.000000</td>\n      <td>303.000000</td>\n      <td>303.000000</td>\n      <td>303.000000</td>\n      <td>303.000000</td>\n      <td>303.000000</td>\n      <td>303.000000</td>\n      <td>303.000000</td>\n      <td>303.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>54.366337</td>\n      <td>0.683168</td>\n      <td>0.966997</td>\n      <td>131.623762</td>\n      <td>246.264026</td>\n      <td>0.148515</td>\n      <td>0.528053</td>\n      <td>149.646865</td>\n      <td>0.326733</td>\n      <td>1.039604</td>\n      <td>1.399340</td>\n      <td>0.729373</td>\n      <td>2.313531</td>\n      <td>0.544554</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>9.082101</td>\n      <td>0.466011</td>\n      <td>1.032052</td>\n      <td>17.538143</td>\n      <td>51.830751</td>\n      <td>0.356198</td>\n      <td>0.525860</td>\n      <td>22.905161</td>\n      <td>0.469794</td>\n      <td>1.161075</td>\n      <td>0.616226</td>\n      <td>1.022606</td>\n      <td>0.612277</td>\n      <td>0.498835</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>29.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>94.000000</td>\n      <td>126.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>71.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>47.500000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>120.000000</td>\n      <td>211.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>133.500000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>2.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>55.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>130.000000</td>\n      <td>240.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>153.000000</td>\n      <td>0.000000</td>\n      <td>0.800000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>2.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>61.000000</td>\n      <td>1.000000</td>\n      <td>2.000000</td>\n      <td>140.000000</td>\n      <td>274.500000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>166.000000</td>\n      <td>1.000000</td>\n      <td>1.600000</td>\n      <td>2.000000</td>\n      <td>1.000000</td>\n      <td>3.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>77.000000</td>\n      <td>1.000000</td>\n      <td>3.000000</td>\n      <td>200.000000</td>\n      <td>564.000000</td>\n      <td>1.000000</td>\n      <td>2.000000</td>\n      <td>202.000000</td>\n      <td>1.000000</td>\n      <td>6.200000</td>\n      <td>2.000000</td>\n      <td>4.000000</td>\n      <td>3.000000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "age          41\nsex           2\ncp            4\ntrtbps       49\nchol        152\nfbs           2\nrestecg       3\nthalachh     91\nexng          2\noldpeak      40\nslp           3\ncaa           5\nthall         4\noutput        2\ndtype: int64"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "age           int64\nsex           int64\ncp            int64\ntrtbps        int64\nchol          int64\nfbs           int64\nrestecg       int64\nthalachh      int64\nexng          int64\noldpeak     float64\nslp           int64\ncaa           int64\nthall         int64\noutput        int64\ndtype: object"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(heart.describe(),  heart.nunique(), heart.dtypes)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "From the dataset, it appears that nominal attributes have already been\n",
    "discretized\n",
    "The features that were originally nominal are\n",
    "1. age - age in years\n",
    "\n",
    "2. sex - sex (1 = male; 0 = female)\n",
    "\n",
    "3. cp - chest pain type (1 = typical angina; 2 = atypical angina; 3 = non-anginal pain; 4 = asymptomatic)\n",
    "\n",
    "4. trestbps - resting blood pressure (in mm Hg on admission to the hospital)\n",
    "\n",
    "5. chol - serum cholestoral in mg/dl\n",
    "\n",
    "6. fbs - fasting blood sugar > 120 mg/dl (1 = true; 0 = false)\n",
    "\n",
    "7. restecg - resting electrocardiographic results (0 = normal; 1 = having ST-T; 2 = hypertrophy)\n",
    "\n",
    "8. thalach - maximum heart rate achieved\n",
    "\n",
    "9. exang - exercise induced angina (1 = yes; 0 = no)\n",
    "\n",
    "10. oldpeak - ST depression induced by exercise relative to rest\n",
    "\n",
    "11. slope - the slope of the peak exercise ST segment (1 = upsloping; 2 = flat; 3 = downsloping)\n",
    "\n",
    "12. ca - number of major vessels (0-3) colored by flourosopy\n",
    "\n",
    "13. thall: Thallium Stress Test result (0-3)\n",
    "\n",
    "14. output - diagnosis of heart disease (angiographic disease status) (Value 0 = < diameter narrowing; Value 1 = > 50% diameter narrowing)\n",
    "#### output: 0 = less chance of heart attack, 1 = more chance of heart attack"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "outputs": [
    {
     "data": {
      "text/plain": "age         0\nsex         0\ncp          0\ntrtbps      0\nchol        0\nfbs         0\nrestecg     0\nthalachh    0\nexng        0\noldpeak     0\nslp         0\ncaa         0\nthall       0\noutput      0\ndtype: int64"
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heart.isnull().sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age [63 37 41 56 57 44 52 54 48 49 64 58 50 66 43 69 59 42 61 40 71 51 65 53\n",
      " 46 45 39 47 62 34 35 29 55 60 67 68 74 76 70 38 77]\n",
      "sex [1 0]\n",
      "cp [3 2 1 0]\n",
      "trtbps [145 130 120 140 172 150 110 135 160 105 125 142 155 104 138 128 108 134\n",
      " 122 115 118 100 124  94 112 102 152 101 132 148 178 129 180 136 126 106\n",
      " 156 170 146 117 200 165 174 192 144 123 154 114 164]\n",
      "chol [233 250 204 236 354 192 294 263 199 168 239 275 266 211 283 219 340 226\n",
      " 247 234 243 302 212 175 417 197 198 177 273 213 304 232 269 360 308 245\n",
      " 208 264 321 325 235 257 216 256 231 141 252 201 222 260 182 303 265 309\n",
      " 186 203 183 220 209 258 227 261 221 205 240 318 298 564 277 214 248 255\n",
      " 207 223 288 160 394 315 246 244 270 195 196 254 126 313 262 215 193 271\n",
      " 268 267 210 295 306 178 242 180 228 149 278 253 342 157 286 229 284 224\n",
      " 206 167 230 335 276 353 225 330 290 172 305 188 282 185 326 274 164 307\n",
      " 249 341 407 217 174 281 289 322 299 300 293 184 409 259 200 327 237 218\n",
      " 319 166 311 169 187 176 241 131]\n",
      "fbs [1 0]\n",
      "restecg [0 1 2]\n",
      "thalachh [150 187 172 178 163 148 153 173 162 174 160 139 171 144 158 114 151 161\n",
      " 179 137 157 123 152 168 140 188 125 170 165 142 180 143 182 156 115 149\n",
      " 146 175 186 185 159 130 190 132 147 154 202 166 164 184 122 169 138 111\n",
      " 145 194 131 133 155 167 192 121  96 126 105 181 116 108 129 120 112 128\n",
      " 109 113  99 177 141 136  97 127 103 124  88 195 106  95 117  71 118 134\n",
      "  90]\n",
      "exng [0 1]\n",
      "oldpeak [2.3 3.5 1.4 0.8 0.6 0.4 1.3 0.  0.5 1.6 1.2 0.2 1.8 1.  2.6 1.5 3.  2.4\n",
      " 0.1 1.9 4.2 1.1 2.  0.7 0.3 0.9 3.6 3.1 3.2 2.5 2.2 2.8 3.4 6.2 4.  5.6\n",
      " 2.9 2.1 3.8 4.4]\n",
      "slp [0 2 1]\n",
      "caa [0 2 1 3 4]\n",
      "thall [1 2 3 0]\n",
      "output [1 0]\n"
     ]
    }
   ],
   "source": [
    "for col in heart.columns:\n",
    "    print(col, heart[col].unique())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Shows that we are not missing any value and values are not out of place"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Formulate a classification problem for dataset & solve using Decision Tree\n",
    "* Use at least 2 different train-test split (0.25 and 0.2)\n",
    "* Use at least 2 different cross-validation options (5 fold and 10 fold)\n",
    "* compare classifier performance"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dataset - Heart Attack Analysis & Prediction dataset\n",
    "* source https://www.kaggle.com/rashikrahmanpritom/heart-attack-analysis-prediction-dataset\n",
    "### Question: Predict output, the chance of heart of heart attack using predictor variables\n",
    "### Methods Used: Code that will be listed below"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "outputs": [],
   "source": [
    "X = heart.drop('output', axis=1)\n",
    "y = heart['output']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.25, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "outputs": [
    {
     "data": {
      "text/plain": "GridSearchCV(cv=5, estimator=DecisionTreeClassifier(random_state=55),\n             param_grid={})"
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(random_state=55)\n",
    "dt_grid = GridSearchCV(dt, {}, cv=5)\n",
    "dt_grid.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score 0.7094685990338163\n",
      "test score 0.8026315789473685\n",
      "std dev 0.0361868963859292\n",
      "Confusion Matrix \n",
      " [[29  9]\n",
      " [ 6 32]] \n",
      "\n",
      "Accuracy Score \n",
      " 0.8026315789473685 \n",
      "\n",
      "Classification Report \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.76      0.79        38\n",
      "           1       0.78      0.84      0.81        38\n",
      "\n",
      "    accuracy                           0.80        76\n",
      "   macro avg       0.80      0.80      0.80        76\n",
      "weighted avg       0.80      0.80      0.80        76\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('train score', dt_grid.best_score_)\n",
    "print('test score', dt_grid.score(X_test, y_test))\n",
    "print('std dev', dt_grid.cv_results_['std_test_score'][dt_grid.best_index_])\n",
    "\n",
    "print('Confusion Matrix \\n',confusion_matrix(dt_grid.predict(X_test),y_test), '\\n')\n",
    "print('Accuracy Score \\n', dt_grid.score(X_test, y_test), '\\n')\n",
    "print('Classification Report \\n',classification_report(dt_grid.predict(X_test),y_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Attempt with different train test split and different cross validation for tree classifier\n",
    "* Test size will be reduce form .25 to .15\n",
    "* Cross validation will increase from 5 to 10"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "outputs": [],
   "source": [
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X,y, test_size=0.20, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "outputs": [
    {
     "data": {
      "text/plain": "GridSearchCV(cv=10, estimator=DecisionTreeClassifier(random_state=55),\n             param_grid={})"
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_grid2 = GridSearchCV(dt, {}, cv=10)\n",
    "dt_grid2.fit(X_train_2, y_train_2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score 0.7436666666666667\n",
      "test score 0.8360655737704918\n",
      "std dev 0.07126944179193023\n",
      "Confusion Matrix \n",
      " [[26  7]\n",
      " [ 3 25]] \n",
      "\n",
      "Accuracy Score \n",
      " 0.8360655737704918 \n",
      "\n",
      "Classification Report \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.79      0.84        33\n",
      "           1       0.78      0.89      0.83        28\n",
      "\n",
      "    accuracy                           0.84        61\n",
      "   macro avg       0.84      0.84      0.84        61\n",
      "weighted avg       0.84      0.84      0.84        61\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('train score', dt_grid2.best_score_)\n",
    "print('test score', dt_grid2.score(X_test_2, y_test_2))\n",
    "print('std dev', dt_grid2.cv_results_['std_test_score'][dt_grid2.best_index_])\n",
    "\n",
    "print('Confusion Matrix \\n',confusion_matrix(dt_grid2.predict(X_test_2),y_test_2), '\\n')\n",
    "print('Accuracy Score \\n', dt_grid2.score(X_test_2, y_test_2), '\\n')\n",
    "print('Classification Report \\n',classification_report(dt_grid2.predict(X_test_2),y_test_2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluation between two tree classifiers\n",
    "From scores of both classifiers suggests underfitting and would probably\n",
    "performed better with standardization and dimensionality reduction of\n",
    " the data before training our model. Also it appears that an\n",
    "increase in train size seems to increase the test score and\n",
    "increase in CV folds seems to increase variability and makes sense\n",
    "as 10 folds for a small size would cause an increase in variance"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Formulate a classification problem for dataset & solve using K-Nearest-Neighbor Method\n",
    "* Use at least 2 different train-test split (0.25 test size and 0.20 test size)\n",
    "* Use at least 2 different cross-validation options (5 fold and 10 fold)\n",
    "* compare classifier performances\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "outputs": [
    {
     "data": {
      "text/plain": "GridSearchCV(cv=5, estimator=KNeighborsClassifier(), param_grid={})"
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is training with .25 test split and .75 train split\n",
    "# train with cv fold = 5\n",
    "knn_grid = GridSearchCV(KNeighborsClassifier(), {}, cv=5)\n",
    "knn_grid.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score 0.6297584541062802\n",
      "test score 0.6973684210526315\n",
      "std dev 0.093212732516713\n",
      "Confusion Matrix \n",
      " [[22 10]\n",
      " [13 31]] \n",
      "\n",
      "Accuracy Score \n",
      " 0.6973684210526315 \n",
      "\n",
      "Classification Report \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.69      0.66        32\n",
      "           1       0.76      0.70      0.73        44\n",
      "\n",
      "    accuracy                           0.70        76\n",
      "   macro avg       0.69      0.70      0.69        76\n",
      "weighted avg       0.70      0.70      0.70        76\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('train score', knn_grid.best_score_)\n",
    "print('test score', knn_grid.score(X_test, y_test))\n",
    "print('std dev', knn_grid.cv_results_['std_test_score'][knn_grid.best_index_])\n",
    "\n",
    "print('Confusion Matrix \\n',confusion_matrix(knn_grid.predict(X_test),y_test), '\\n')\n",
    "print('Accuracy Score \\n', knn_grid.score(X_test, y_test), '\\n')\n",
    "print('Classification Report \\n',classification_report(knn_grid.predict(X_test),y_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Attempt with different train test split and different cross validation for KNN classifier\n",
    "* Test size will be reduce form .25 to .15\n",
    "* Cross validation will increase from 5 to 10"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "outputs": [
    {
     "data": {
      "text/plain": "GridSearchCV(cv=10, estimator=KNeighborsClassifier(), param_grid={})"
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is training with .20 test split and .80 train split\n",
    "# train with cv fold = 3\n",
    "knn_grid2 = GridSearchCV(KNeighborsClassifier(), {}, cv=10)\n",
    "knn_grid2.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score 0.6268774703557313\n",
      "test score 0.6885245901639344\n",
      "std dev 0.12030623280385462\n",
      "Confusion Matrix \n",
      " [[18  8]\n",
      " [11 24]] \n",
      "\n",
      "Accuracy Score \n",
      " 0.6885245901639344 \n",
      "\n",
      "Classification Report \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.69      0.65        26\n",
      "           1       0.75      0.69      0.72        35\n",
      "\n",
      "    accuracy                           0.69        61\n",
      "   macro avg       0.69      0.69      0.69        61\n",
      "weighted avg       0.69      0.69      0.69        61\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('train score', knn_grid2.best_score_)\n",
    "print('test score', knn_grid2.score(X_test_2, y_test_2))\n",
    "print('std dev', knn_grid2.cv_results_['std_test_score'][knn_grid2.best_index_])\n",
    "\n",
    "print('Confusion Matrix \\n',confusion_matrix(knn_grid2.predict(X_test_2),y_test_2), '\\n')\n",
    "print('Accuracy Score \\n', knn_grid2.score(X_test_2, y_test_2), '\\n')\n",
    "print('Classification Report \\n',classification_report(knn_grid2.predict(X_test_2),y_test_2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluation comparision between two KNN classifiers\n",
    "\n",
    "Between the two KNN classifier it appears that increasing the number of folds and\n",
    "increasing the train size had decreased the performance of the model in all aspects."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using same dataset as earlier but changing question\n",
    "### Dataset - Heart Attack Analysis & Prediction dataset\n",
    "* source https://www.kaggle.com/rashikrahmanpritom/heart-attack-analysis-prediction-dataset\n",
    "### Question: Predict chol - serum cholestorol in patients using predictor variables\n",
    "### Methods Used: Code that will be listed below"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Formulate a Numerical prediction for dataset & solve using Linear Regression\n",
    "* Use at least 2 different train-test split (0.25 test size and 0.50 test size)\n",
    "* Use at least 2 different cross-validation options (5 fold and 3 fold)\n",
    "* compare regressor performances"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "outputs": [],
   "source": [
    "X = heart.drop('chol', axis=1)\n",
    "y = heart['chol']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "outputs": [],
   "source": [
    "lr_pipe = Pipeline([('scaler', StandardScaler()), ('lr', LinearRegression())])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "outputs": [
    {
     "data": {
      "text/plain": "GridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n                                       ('lr', LinearRegression())]),\n             param_grid={})"
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_grid = GridSearchCV(lr_pipe, {}, cv=5)\n",
    "lr_grid.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score -0.05932573916186505\n",
      "test score -0.1603538511720659\n",
      "std dev 0.25588682979376504\n"
     ]
    }
   ],
   "source": [
    "print('train score', lr_grid.best_score_)\n",
    "print('test score', lr_grid.score(X_test, y_test))\n",
    "print('std dev', lr_grid.cv_results_['std_test_score'][lr_grid.best_index_])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Try different train-test-splits and cv folds for Linear Regressoin\n",
    "* test size change from 0.25 to 0.50\n",
    "* CV fold change form 5 to 3"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "outputs": [],
   "source": [
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X, y, test_size=0.5, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "outputs": [
    {
     "data": {
      "text/plain": "GridSearchCV(cv=3,\n             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n                                       ('lr', LinearRegression())]),\n             param_grid={})"
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_grid2 = GridSearchCV(lr_pipe, {}, cv=3)\n",
    "lr_grid2.fit(X_train_2, y_train_2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score -0.4324609448109374\n",
      "test score -0.32419777529413873\n",
      "std dev 0.7317258951202023\n"
     ]
    }
   ],
   "source": [
    "print('train score', lr_grid2.best_score_)\n",
    "print('test score', lr_grid2.score(X_test_2, y_test_2))\n",
    "print('std dev', lr_grid2.cv_results_['std_test_score'][lr_grid2.best_index_])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluation between linear regressors\n",
    "From this dataset it seems trying to predict cholestorl with the predictor variables\n",
    "does a very poor job\n",
    "And in comparing the two regressors it appears that increasing test size and reducing number\n",
    "of folds did even worse than the poor score that was orignally had."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Formulate a Numerical prediction for dataset & solve using Non-Linear Regression\n",
    "* Use at least 2 different train-test split\n",
    "* Use at least 2 different cross-validation options\n",
    "* compare regressor performances\n",
    "\n",
    "\n",
    "* Support Vector Regression will be used with a poly kernel\n",
    "* first evaluation will have .25 test size and 5 folds\n",
    "* Same question of predicting cholesterol will be tested but using different model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "outputs": [],
   "source": [
    "svr_pipe = Pipeline([('scaler', StandardScaler()), ('svr', SVR(kernel='poly'))])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "outputs": [
    {
     "data": {
      "text/plain": "GridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n                                       ('svr', SVR(kernel='poly'))]),\n             param_grid={})"
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svr_grid = GridSearchCV(svr_pipe, {}, cv=5)\n",
    "svr_grid.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score -0.11551545630340895\n",
      "test score -0.030530582221110825\n",
      "std dev 0.09320008024184157\n"
     ]
    }
   ],
   "source": [
    "print('train score', svr_grid.best_score_)\n",
    "print('test score', svr_grid.score(X_test, y_test))\n",
    "print('std dev', svr_grid.cv_results_['std_test_score'][svr_grid.best_index_])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Try different train-test-splits and cv folds for Linear Regressoin\n",
    "* test size change from 0.25 to 0.30\n",
    "* CV fold change form 5 to 3"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "outputs": [],
   "source": [
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X, y, test_size=0.3, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "outputs": [
    {
     "data": {
      "text/plain": "GridSearchCV(cv=3,\n             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n                                       ('svr', SVR(kernel='poly'))]),\n             param_grid={})"
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svr_grid2 = GridSearchCV(svr_pipe, {}, cv=3)\n",
    "svr_grid2.fit(X_train_2, y_train_2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score -0.03387384897510252\n",
      "test score 0.004391712555274707\n",
      "std dev 0.040489614501586786\n"
     ]
    }
   ],
   "source": [
    "print('train score', svr_grid2.best_score_)\n",
    "print('test score', svr_grid2.score(X_test_2, y_test_2))\n",
    "print('std dev', svr_grid2.cv_results_['std_test_score'][svr_grid2.best_index_])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluation between nonlinear regressors\n",
    "It appears that increasing test size and decraasing number of folds increased\n",
    "performance slightly although the relationship between our target for cholestorol\n",
    "and the predictor variables appear to be non-existant. Also svr also performed better\n",
    "than the linear model; but both models show a poor or nonexistant relationship\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}